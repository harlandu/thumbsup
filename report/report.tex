\documentclass[letterpaper]{article}

% NIPS header
\usepackage{nips10submit_e,times} 
\usepackage{helvet} 
\usepackage{courier}
\usepackage{url}

% fancy symbols
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
% \usepackage{natbib}
\usepackage{graphicx}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}

\title{Classifying reviews}
\author{Ertan Dogrultan, Cesar Romero, Paul Wais\\
Computer Science Department \\
University of California, Los Angeles\\
Los Angeles, California 90095\\
\texttt{\{ertan,romero\}@cs.ucla.edu, pwais@ucla.edu}}

\nipsfinalcopy

\begin{document}
\maketitle
\begin{abstract}
  We have an awesome translator that works for amazon and yelp reviews.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

We tell a story to motivate the problem. We mention Jenn's
work~\cite{JennLearnDiffDomains} and the sentiment classification
work~\cite{PangSentimentClassification}.

\section{Feature Extraction and Labelling the Data}
We have divided the features into several categories.
\subsection{Features}
\subsubsection{Word Frequencies and Counts}
Our simplest feature is the number of words in the review. Similarly,
we calculate the average number of words per sentence and
total number of sentences in the review. Furthermore, we count the
number of URLs.

We obtain many different sets of words divided with respect to their
contextual meanings i.e. time, space, comparison, contrast, summary,
suggestion, emphasis, etc. and we count the number of occurences of
these in a review. For instance, comparison words are
\emph{similarly, likewise,} etc., contrast words are \emph{but,
  however, nevertheless, in spite of,} etc. Moreover, we have some
other sets of special words such as SAT, GRE lists on which we apply the same procedure.  We normalize by dividing each of them by the total
number of words in the body of the review and use them as features.   

\subsubsection{Sentential Errors}
We count the grammar and spelling errors with libraries using
Microsoft Word. In addition to those, we get the number of
capitalization mistakes and words with all capital letters. Similarly, we
divide those count by the total number of words in the review to have
a better sense of measurement.  

\subsubsection{Scores}

\subsubsection{Product/Service Related Features} 
This feature is somewhat different than the previous ones
because it depends on the product as opposed to a particular
review. We include the price of the product from the Amazon data set and
discretize it so that we have a corresponding measurement of price
range as we have in Yelp reviews (an integer between 1-4). 

\subsection{Deciding on the Labelling}
In the early experiments, we used$50^{th}$ percentile cutoff to label the
review as useful. In other words, if the positive votes for a review is above
$50^{th}$ percentile, we labeled it as useful. However, this approach affected our classification
accuracy. Therefore, we pushed the thresholds to more extreme values
to get rid of the noisy data in the middle. Currently, we label a review as useful
if its positive votes are in the $75^{th}$ percentile or above and not
useful if they are 
in the $5^{th}$ percentile or below. We observe the reflection of this
approach on the confusion matrices. We will discuss more about this in
Section \ref{sec:single_domain}.


\section{Single Domain Experiments}
\label{sec:single_domain}  



\section{Domain Adaptation}
\label{sec:background}

\emph{Here, we explain what domain adaptation is and its important
prameters.}
We are interested in being able to classify reviews in general - not
just from a specific source or category. A natural problem to consider
is Domain Adaptation. In this setting, there is a source domain $S$
and a target domain $T$. The goal is to be able to classify reviews in
$T$ when most of our data comes from $S$. 

Include the fancy Theorem here that will be used to predict the error
of the calssifiers in the domain adaptation setting.

\subsection{$\alpha$-error}
\label{sec:alpha-error}

Explain the $\alpha$-error here.

\subsection{Estimating the distance}
\label{sec:estimating-distance}

We train a linear classifier to learn to which domain a review
belongs. The error of this classifier is used as an estimate of the
distance between the distributions.

\section{Mid-quarter report}
\label{sec:mid-quarter-report}

In the report we cited~\cite{citeulike:352583} among others. Figure~

\begin{figure*}[h]
  \centering
  \includegraphics[scale=.5]{features_distributions}
  \caption{This is the figure from the report}
  \label{fig:dist}
\end{figure*}

\section{Features}
\label{sec:features}

A concise and example-driven description of all our features.  This
will probably take a little while to write :). And we should probably
have a reference to ANEW~\cite{DoddsANEWPaper}.

Link to our github:
\url{https://github.com/pwais/thumbsup}

\section{Experiments}
\label{sec:experiments}

We present two sets of experiments. In the first set, we use different
models to learn the quality of the review using the features described
in~\ref{sec:features}. Then, we use the model that performs best in
the first set of experiments to classify reviews in a domain
adaptation setting. We use the WEKA machine learning suite for our
experiments~\cite{weka}.

\subsection{Learning Reviews}
\label{sec:learning-reviews}

We use three different models to learn the quality of the reviews:
Support Vector MAchines (SVMs), Adaboost, and Naive Bayes.

\emph{Our original data includes a lot of noise. We can probably talk
  here about taking only the extreme cases: either very good (over
  85th percentile) or very bad (under 5th percentile).}

\subsection{Domain Adaptation}
\label{sec:domain-adaptation}

In this set of experiments we use the \emph{insert model here} to
learn from both domains. For this experiments, Yelp is considered to be
the \emph{target} and Amazon the \emph{source}.

We estimate the distance between the distributions, $\zeta$ , using the metod
previously explained in the domain adaptation Section. For the first
set of domain adaptation experimentes, we vary the value of $\beta \in
\{0.1, 0.2, 0.4, 0.8\}$ while holding $\zeta$ constant.

Using a fixed $\beta$ we want see how well the classifier performs
when the distance between the target and source distribution
varies. To alter $\zeta$, we consider only a specific category on each
domain. From the source domain (Amazon), we consider the set of
categories $C_S=\{$ Books, Electronics, DVDs and Clothing\}. From the
target domain (Yelp) we consider only one category
$C_T=$\{Restaurants\}. For each combination in $C_s\times C_T$ we train
and test a different classifier. Notice that, to estimate the distance
between any of the two categories in the four possible combinations,
we need to train a separate linear classifier to estimate the distance
as we did before. Figure~\ref{fig:domain-adaptation} shows the errors
of these classifiers together with their theoretical bounds.

\begin{figure}
  \centering
  
  \caption{Domain Adaptation Experiments}
  \label{fig:domain-adaptation}
\end{figure}

\section{Citations to use}
Jenn's domain adaption paper \cite{JennLearnDiffDomains}\\
ANEW paper \cite{DoddsANEWPaper}\\
Pang et al sentiment classification \cite{PangSentimentClassification}\\
The WEKA paper that they ask users to cite \cite{weka} \\

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bib}

\end{document}
